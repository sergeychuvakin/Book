{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[7.0065e-45, 0.0000e+00]])"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "torch.empty(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[0.0949, 0.0394, 0.9195],\n        [0.7508, 0.6584, 0.4582],\n        [0.9770, 0.2864, 0.5772]])"
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "torch.rand(3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0]])"
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "torch.zeros(5, 3, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([5.9000, 3.0000])"
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "source": [
    "torch.tensor([5.9, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([5.9, 3]).new_ones(4, 3, dtype=torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[-0.2066, -0.5040, -0.4189],\n        [-0.6668, -1.1029, -1.2669],\n        [-0.0884, -0.8044,  0.7388],\n        [ 0.2915, -0.4512,  0.8479]])\n"
    }
   ],
   "source": [
    "x = torch.randn_like(x, dtype=torch.float) \n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[ 0.5569, -0.0403,  0.2934],\n        [-0.1717, -0.1549, -0.5790],\n        [ 0.5355, -0.7782,  0.8378],\n        [ 0.4211,  0.2354,  1.0857]])"
     },
     "metadata": {},
     "execution_count": 90
    }
   ],
   "source": [
    "torch.rand(4, 3) + x\n",
    "torch.rand(4, 3).add(x) # the same\n",
    "\n",
    "# another way\n",
    "\n",
    "y = torch.rand(4, 3)\n",
    "torch.add(x, y, out=y)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[-0.2066, -0.5040, -0.4189],\n        [-0.6668, -1.1029, -1.2669]])"
     },
     "metadata": {},
     "execution_count": 96
    }
   ],
   "source": [
    "x[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[-0.2066, -0.5040],\n        [-0.4189, -0.6668],\n        [-1.1029, -1.2669],\n        [-0.0884, -0.8044],\n        [ 0.7388,  0.2915],\n        [-0.4512,  0.8479]])"
     },
     "metadata": {},
     "execution_count": 102
    }
   ],
   "source": [
    "# chage shape\n",
    "x.view(6,2)\n",
    "x.view(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "False"
     },
     "metadata": {},
     "execution_count": 104
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[1., 1.],\n        [1., 1.]], requires_grad=True)\ntensor([[1., 1.],\n        [1., 1.]])\n"
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "x1 = torch.ones(2, 2)\n",
    "print(x)\n",
    "print(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<AddBackward0 at 0x1203bf310>"
     },
     "metadata": {},
     "execution_count": 112
    }
   ],
   "source": [
    "(x + 2).grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<MulBackward0 at 0x120022fd0>"
     },
     "metadata": {},
     "execution_count": 121
    }
   ],
   "source": [
    "x1 = x1 * x1 * 3\n",
    "x1.requires_grad_(True).grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pickle(file_name, obj):\n",
    "    import pickle \n",
    "    with open(\"%s.pickle\" % file_name, 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def read_pickle(file_name):\n",
    "    import pickle \n",
    "    with open(\"%s.pickle\" % file_name, 'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pickle('my_name', x)\n",
    "y = read_pickle('my_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1"
     },
     "metadata": {},
     "execution_count": 128
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch tutorial\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([10.,  3.])\n"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "x = torch.Tensor([5, 3])\n",
    "y = torch.Tensor([2, 1])\n",
    "\n",
    "print(x*y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[0.5046, 0.5092, 0.9381, 0.6743, 0.9893, 0.3016, 0.4624, 0.6641, 0.5811,\n         0.4150]])\n"
    }
   ],
   "source": [
    "x = torch.zeros([2,5])\n",
    "y = torch.rand([2,5])\n",
    "print(y.view([1, 10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision \n",
    "from torchvision import transforms, datasets\n",
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = datasets.MNIST('', train=True, download=True, \\\n",
    "                    transform=transforms.Compose([transforms.ToTensor()]))\n",
    "test = datasets.MNIST('', train=False, download=True, \\\n",
    "                    transform=transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torch.utils.data.DataLoader(train, batch_size=10, shuffle=True)\n",
    "testset = torch.utils.data.DataLoader(test, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.]]],\n\n\n        [[[0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.]]],\n\n\n        [[[0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.]]],\n\n\n        ...,\n\n\n        [[[0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.]]],\n\n\n        [[[0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.]]],\n\n\n        [[[0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([2, 9, 6, 1, 1, 5, 0, 5, 9, 5])]\n"
    }
   ],
   "source": [
    "for i in trainset:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y  = i[0][0], i[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Counter({6: 5918,\n         0: 5923,\n         1: 6742,\n         8: 5851,\n         2: 5958,\n         3: 6131,\n         4: 5842,\n         5: 5421,\n         7: 6265,\n         9: 5949})"
     },
     "metadata": {},
     "execution_count": 104
    }
   ],
   "source": [
    "from collections import Counter\n",
    "_c = Counter(int(i) for ii in list(df[1]) for i in ii) # or i.item()\n",
    "_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 251.565 248.518125\" width=\"251.565pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 248.518125 \nL 251.565 248.518125 \nL 251.565 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 26.925 224.64 \nL 244.365 224.64 \nL 244.365 7.2 \nL 26.925 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p29138c7df3)\">\n    <image height=\"218\" id=\"image2ca85cf9fc\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"26.925\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAANoAAADaCAYAAADAHVzbAAAABHNCSVQICAgIfAhkiAAABgxJREFUeJzt3VuInPUdxvF3NmNzQFMP0ZrYeKBoq+KFYEtdqRSM0kLvjFotRUSwWEURRMGAh1uLVGgMtEWhBLwRUURoCRrohW0qHsAqrsSiiBceWqKrySZxdqd33uj7m7jv7rO7k8/n9nHeeWP8+sL+mdnelt7WYQMsqomlvgE4GggNAoQGAUKDAKFBgNAgQGgQIDQIEBoECA0ChAYBQoMAoUGA0CBAaBAgNAgQGgQIDQKEBgFCgwChQYDQIEBoECA0CBAaBAgNAoQGAUKDAKFBgNAgQGgQIDQIEBoECA0ChAYBQoMAoUGA0CCgv9Q3MK5WnXRi6zb907MX9b2P2/tpuc+9NrWo789XeaJBgNAgQGgQIDQIEBoECA0ChAYBY3uOtmr9+nLfu+38cp89dq7cH75iZ7mvnzjYuv1kzXPla7v6z2Cm3KcOn9y6rerVf+63Dm4q9+27Ly/3yrc+qf+/f+ZT0/O+dtM0zcSBQ+U++9bbna5fvveiXRn4ktAgQGgQIDQIEBoECA0ChAYBvS29rcOlvonFMHju9HLfde7ToTthuXh3cKDcf//RZfO+9t4f1md0nmgQIDQIEBoECA0ChAYBQoOAsf2YzPPnPVPus4t8qHHl2z9v3WYGxyzum3fQ69X/Yjas2V/u156yp9x/trb+EXsXf5tZV+6b+4fLfe90+8eHRnu/XD3RIEBoECA0CBAaBAgNAoQGAUKDgLE9R5sd1l+b1tWFD91a7hsf/lf7ODe7wHezcEYdL/63X/8n88hxF5f7jnVrv+EdHbnhgfpr9poR9958XJ+FdeGJBgFCgwChQYDQIEBoECA0CBAaBIztOdrZT95c7m9eub3c+82qch9M1r9CaOKPa1q3uf31Z7qWs+FgUO6z+/bVFxi1jylPNAgQGgQIDQKEBgFCgwChQYDQIGBsf23TKGe+WH8uasdpL3S6/o8euKV12/Cnf3a6NiuPJxoECA0ChAYBQoMAoUGA0CDgqP3xfv+0TeX+6931j+CvOvZ/5T49d7B1u/QPd5av3fTgP8qdlccTDQKEBgFCgwChQYDQIEBoECA0CDhqz9FG+fzqH5f7L+/7a7n/9vh3Wrc3vjhcvnbr43eU+1nb9pR7M/RXutx4okGA0CBAaBAgNAgQGgQIDQKEBgHO0eZp+rr6nO26be3nbNUZ25E4/9H2r7JrmqY5415fZ7fceKJBgNAgQGgQIDQIEBoECA0ChAYBztEWyfv3TLZuz/7mwfK1p/fXlfueQ/V733/DjeU+8fdX6wuw4DzRIEBoECA0CBAaBAgNAoQGAUKDAOdoS+C9+9rP2JqmaV6/aXun6//5083l/tR5J3e6Pt+cJxoECA0ChAYBQoMAoUGA0CDAj/eXQG/16nJ/d+c55f7GJX8p931zM+V+9Y23t27H7HqpfC3z44kGAUKDAKFBgNAgQGgQIDQIEBoE9Jf6Bo5Gw0P198V9Z+facj80OSj3Eybq139w08HWbfOu8qXMkycaBAgNAoQGAUKDAKFBgNAgQGgQ4PNoK9B7T1xQ7q9Pzv/zar/afMm87omaJxoECA0ChAYBQoMAoUGA0CBAaBDg82gr0LDjyeeoz6ux8DzRIEBoECA0CBAaBAgNAoQGAUKDgLE9R/viiovK/bPbpsv9wAsbyn3mB+3fjdg0TfO9735c7l288v1HR/wT9V/rk/tPWLib4Yh4okGA0CBAaBAgNAgQGgQIDQLG9sf7zz62o9xX90b80S9cwJtZcN3+2u7efU3rdk7zYqdr8/U80SBAaBAgNAgQGgQIDQKEBgFCg4CxPUe758PJcn/o1JV7XlT92qWmaZpf/Pv6cj/3rqnWbXZed8QonmgQIDQIEBoECA0ChAYBQoMAoUFAb0tva8dfArQ89TeeWu5Tv9tY7mvWHu70/sNXv926nTjV7bRq9b5Bufeff7nT9Vl4nmgQIDQIEBoECA0ChAYBQoMAoUHA2J6jwXLiiQYBQoMAoUGA0CBAaBAgNAgQGgQIDQKEBgFCgwChQYDQIEBoECA0CBAaBAgNAoQGAUKDAKFBgNAgQGgQIDQI+D/l+tzYxlQf0QAAAABJRU5ErkJggg==\" y=\"-6.64\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"mfa65889c78\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.807857\" xlink:href=\"#mfa65889c78\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(27.626607 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"69.636429\" xlink:href=\"#mfa65889c78\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 5 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(66.455179 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"108.465\" xlink:href=\"#mfa65889c78\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 10 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(102.1025 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"147.293571\" xlink:href=\"#mfa65889c78\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 15 -->\n      <g transform=\"translate(140.931071 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"186.122143\" xlink:href=\"#mfa65889c78\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 20 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(179.759643 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"224.950714\" xlink:href=\"#mfa65889c78\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 25 -->\n      <g transform=\"translate(218.588214 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"mb7baed86a0\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mb7baed86a0\" y=\"11.082857\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0 -->\n      <g transform=\"translate(13.5625 14.882076)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mb7baed86a0\" y=\"49.911429\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 5 -->\n      <g transform=\"translate(13.5625 53.710647)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mb7baed86a0\" y=\"88.74\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 10 -->\n      <g transform=\"translate(7.2 92.539219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mb7baed86a0\" y=\"127.568571\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 15 -->\n      <g transform=\"translate(7.2 131.36779)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mb7baed86a0\" y=\"166.397143\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 20 -->\n      <g transform=\"translate(7.2 170.196362)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mb7baed86a0\" y=\"205.225714\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 25 -->\n      <g transform=\"translate(7.2 209.024933)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 26.925 224.64 \nL 26.925 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 244.365 224.64 \nL 244.365 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 26.925 224.64 \nL 244.365 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 26.925 7.2 \nL 244.365 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p29138c7df3\">\n   <rect height=\"217.44\" width=\"217.44\" x=\"26.925\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(x.view(28, 28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 64) ## 28*28 image size\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 10) # 10 classes\n",
    "    \n",
    "    def forward(self, x): # x data\n",
    "        x = F.relu(self.fc1(x)) # F - activation function (rule - kind if sigmoid from zero to one)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim=1) # # normalizes values along axis 1 (row (axis=0) or column (axis=1) that sum to 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Net(\n  (fc1): Linear(in_features=784, out_features=64, bias=True)\n  (fc2): Linear(in_features=64, out_features=64, bias=True)\n  (fc3): Linear(in_features=64, out_features=64, bias=True)\n  (fc4): Linear(in_features=64, out_features=10, bias=True)\n)\n"
    }
   ],
   "source": [
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand((28, 28))\n",
    "X = X.view(1, 28*28)\n",
    "output = net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[-2.2943, -2.2019, -2.2852, -2.3624, -2.2208, -2.3478, -2.3338, -2.2777,\n         -2.3371, -2.3809]], grad_fn=<LogSoftmaxBackward>)"
     },
     "metadata": {},
     "execution_count": 120
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.0001"
     },
     "metadata": {},
     "execution_count": 138
    }
   ],
   "source": [
    "1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch: 0, LOSS: 0.13743475079536438\nEpoch: 1, LOSS: 0.006328423973172903\nEpoch: 2, LOSS: 0.07568399608135223\n"
    }
   ],
   "source": [
    "import torch.optim as optim # to find weights\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-3) # net.parameters() what should be optimized; \n",
    "# lr - learning rate (size of the step in grad. descent)\n",
    "epochs = 3\n",
    "for e in range(epochs):\n",
    "    for data in trainset: ## some kinf of batching \n",
    "        # data is a batch of features and labels \n",
    "        X, y = data\n",
    "        net.zero_grad() # clear grad for every batch\n",
    "        output = net(X.view(-1, 28*28)) ## pass the data throught the net  (NOTE: -1 should be passed instead of 1) \n",
    "        loss = F.nll_loss(output, y) ## loss function (if output not OHE vector use nll_loss, otherwith use Mean Root Squared Error) \n",
    "        loss.backward() # backpropagation of loss\n",
    "        optimizer.step() ## update weights\n",
    "    print(f'Epoch: {e}, LOSS: {loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor(0.0757, grad_fn=<NllLossBackward>)"
     },
     "metadata": {},
     "execution_count": 143
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy: 0.97\n"
    }
   ],
   "source": [
    "## evaluation\n",
    "correct = 0 \n",
    "total = 0 \n",
    "\n",
    "# model.train()\" and \"model.eval()\" activates and deactivates Dropout and BatchNorm, so it is quite important. \"with torch.no_grad()\" only deactivates gradient calculations, but doesn't turn off Dropout and BatchNorm. Your model accuracy will therefore be lower if you don't use model.eval() when evaluating the model.\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in trainset: ### there should be testset \n",
    "        X, y = data\n",
    "        output = net(X.view(-1, 784))\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "            total +=1\n",
    "print(f\"Accuracy: {round(correct/total, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor(3)"
     },
     "metadata": {},
     "execution_count": 150
    }
   ],
   "source": [
    "y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragram_embeddings = np.random.randn(120000,300)\n",
    "glove_embeddings = np.random.randn(120000,300)\n",
    "embedding_matrix = np.mean([glove_embeddings, paragram_embeddings], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "with open('fin.pickle', 'rb') as f: \n",
    "    df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "              score        _class  \\\n0    4703706.0.8690       STUPEFY   \n1    4593427.0.1815         ACCIO   \n2    4278446.0.2692  EXPELLIARMUS   \n3   9507233.0.61523       PROTEGO   \n4  11351772.0.14504         LUMOS   \n\n                                                text         0         1  \\\n0  we watched his inglorious withdrawal together ...  0.008347 -0.053040   \n1  my wand , `` incendio . '' this wretched chill...  0.000724 -0.053348   \n2  already compared ours , they 're the same ever...  0.013692 -0.106544   \n3  his wounds were removed . `` do n't worry , do...  0.070034 -0.135037   \n4  would he care ? and harry vanished , refusing ... -0.007617 -0.070526   \n\n          2         3         4         5         6  ...       290       291  \\\n0 -0.230350  0.006090  0.023212 -0.035828  0.000337  ...  0.038344 -0.098733   \n1 -0.198696  0.017975  0.021162 -0.006253 -0.000227  ...  0.030515 -0.095899   \n2 -0.247722  0.004252 -0.032518 -0.045397  0.016788  ...  0.015337 -0.073834   \n3 -0.195022 -0.029645  0.003196 -0.012578  0.040316  ... -0.076753 -0.125019   \n4 -0.245650 -0.005221 -0.001592 -0.019080  0.003828  ...  0.054683 -0.088570   \n\n        292       293       294       295       296       297       298  \\\n0 -0.002843 -0.103207 -0.033448  0.172728 -0.035697  0.060077 -0.010933   \n1  0.026130 -0.079410 -0.025872  0.215718 -0.050384  0.093738 -0.007752   \n2  0.011775 -0.069102 -0.051675  0.215375 -0.026393  0.053807 -0.033142   \n3 -0.002307 -0.077470 -0.041777  0.207217 -0.011403  0.128364  0.013500   \n4  0.006394 -0.092155 -0.021292  0.165983 -0.008104  0.064210 -0.011174   \n\n        299  \n0  0.068320  \n1  0.102257  \n2  0.150730  \n3  0.162325  \n4  0.099386  \n\n[5 rows x 303 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>score</th>\n      <th>_class</th>\n      <th>text</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>...</th>\n      <th>290</th>\n      <th>291</th>\n      <th>292</th>\n      <th>293</th>\n      <th>294</th>\n      <th>295</th>\n      <th>296</th>\n      <th>297</th>\n      <th>298</th>\n      <th>299</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4703706.0.8690</td>\n      <td>STUPEFY</td>\n      <td>we watched his inglorious withdrawal together ...</td>\n      <td>0.008347</td>\n      <td>-0.053040</td>\n      <td>-0.230350</td>\n      <td>0.006090</td>\n      <td>0.023212</td>\n      <td>-0.035828</td>\n      <td>0.000337</td>\n      <td>...</td>\n      <td>0.038344</td>\n      <td>-0.098733</td>\n      <td>-0.002843</td>\n      <td>-0.103207</td>\n      <td>-0.033448</td>\n      <td>0.172728</td>\n      <td>-0.035697</td>\n      <td>0.060077</td>\n      <td>-0.010933</td>\n      <td>0.068320</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4593427.0.1815</td>\n      <td>ACCIO</td>\n      <td>my wand , `` incendio . '' this wretched chill...</td>\n      <td>0.000724</td>\n      <td>-0.053348</td>\n      <td>-0.198696</td>\n      <td>0.017975</td>\n      <td>0.021162</td>\n      <td>-0.006253</td>\n      <td>-0.000227</td>\n      <td>...</td>\n      <td>0.030515</td>\n      <td>-0.095899</td>\n      <td>0.026130</td>\n      <td>-0.079410</td>\n      <td>-0.025872</td>\n      <td>0.215718</td>\n      <td>-0.050384</td>\n      <td>0.093738</td>\n      <td>-0.007752</td>\n      <td>0.102257</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4278446.0.2692</td>\n      <td>EXPELLIARMUS</td>\n      <td>already compared ours , they 're the same ever...</td>\n      <td>0.013692</td>\n      <td>-0.106544</td>\n      <td>-0.247722</td>\n      <td>0.004252</td>\n      <td>-0.032518</td>\n      <td>-0.045397</td>\n      <td>0.016788</td>\n      <td>...</td>\n      <td>0.015337</td>\n      <td>-0.073834</td>\n      <td>0.011775</td>\n      <td>-0.069102</td>\n      <td>-0.051675</td>\n      <td>0.215375</td>\n      <td>-0.026393</td>\n      <td>0.053807</td>\n      <td>-0.033142</td>\n      <td>0.150730</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9507233.0.61523</td>\n      <td>PROTEGO</td>\n      <td>his wounds were removed . `` do n't worry , do...</td>\n      <td>0.070034</td>\n      <td>-0.135037</td>\n      <td>-0.195022</td>\n      <td>-0.029645</td>\n      <td>0.003196</td>\n      <td>-0.012578</td>\n      <td>0.040316</td>\n      <td>...</td>\n      <td>-0.076753</td>\n      <td>-0.125019</td>\n      <td>-0.002307</td>\n      <td>-0.077470</td>\n      <td>-0.041777</td>\n      <td>0.207217</td>\n      <td>-0.011403</td>\n      <td>0.128364</td>\n      <td>0.013500</td>\n      <td>0.162325</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11351772.0.14504</td>\n      <td>LUMOS</td>\n      <td>would he care ? and harry vanished , refusing ...</td>\n      <td>-0.007617</td>\n      <td>-0.070526</td>\n      <td>-0.245650</td>\n      <td>-0.005221</td>\n      <td>-0.001592</td>\n      <td>-0.019080</td>\n      <td>0.003828</td>\n      <td>...</td>\n      <td>0.054683</td>\n      <td>-0.088570</td>\n      <td>0.006394</td>\n      <td>-0.092155</td>\n      <td>-0.021292</td>\n      <td>0.165983</td>\n      <td>-0.008104</td>\n      <td>0.064210</td>\n      <td>-0.011174</td>\n      <td>0.099386</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 303 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Text(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        filter_sizes = [1,2,3,5]\n",
    "        num_filters = 36\n",
    "        self.embedding = nn.Embedding(max_features, embed_size)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.convs1 = nn.ModuleList([nn.Conv2d(1, num_filters, (K, embed_size)) for K in filter_sizes])\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc1 = nn.Linear(len(Ks)*num_filters, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  \n",
    "        x = x.unsqueeze(1)  \n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1] \n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  \n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.dropout(x)  \n",
    "        logit = self.fc1(x)  \n",
    "        return logit"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bitnnvenv843dd4c5400b4c24a85d73424f1df2db",
   "display_name": "Python 3.7.7 64-bit ('nn': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}